{"cells":[{"metadata":{},"cell_type":"markdown","source":"# #3|Clickbait Headline Classification\n---\n### Clickbait\n\nClickbait, a form of false advertisement, uses hyperlink text or a thumbnail link that is designed to attract attention and to entice users to follow that link and read, view, or listen to the linked piece of online content, with a defining characteristic of being deceptive, typically sensationalized or misleading.\n\n<img src=\"https://www.newswire.com/blog/wp-content/uploads/2019/07/Highres_Adelaine-Emoji-Clickbait.gif\" height=300>\n\n\nSo today we will be dealing with click-bait data consisting of headlines labeled as clickbait or not.\n\n## Let's Get Started.\n----"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/clickbait-dataset/clickbait_data.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we have about 32000 headlines already labeled as true or false for a clickbait (1=True, 0=False).\n\nBut we would be using more data from few other kaggle datasets to build a more precise and stable classifier.\nOther Datasets:-\n### [Fake News Data](https://www.kaggle.com/antmarakis/fake-news-data)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets add and import our new data\nfake_news_df = pd.read_csv('../input/fake-news-data/fnn_politics_fake.csv')\nfake_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we have a quite a few columns here we won't be needing all of them so lets drop some."},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news_df = fake_news_df.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nfake_news_df['clickbait'] = 1\nfake_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so here we have got more click bait headlines lets have some real headlines"},{"metadata":{"trusted":true},"cell_type":"code","source":"real_news_df = pd.read_csv('../input/fake-news-data/fnn_politics_real.csv')\nreal_news_df = real_news_df.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nreal_news_df['clickbait'] = 0\nreal_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so now we have more non click-bait headlines\n\nLet's put all data together and see if we got duplicates and enough data too"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.append(real_news_df, ignore_index = True)\ndf = df.append(fake_news_df, ignore_index = True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we have increased our data with 1056 rows. Lets check and remove the duplicate values if present."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(keep=False, inplace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of rows decreased to 32944 dropping 112 duplicates "},{"metadata":{},"cell_type":"markdown","source":"#### Let's check for number of clickbait headlines and real headlines."},{"metadata":{},"cell_type":"markdown","source":"Let's Add another column with human readable values such as 'clickbait' or 'real' as our categorical features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['category'] = df['clickbait']\ndf['category'] = df['category'].map({1: 'Clickbait', 0: 'Real_News'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = df.clickbait.value_counts()\nprint('''\nNumber of Clickbait headlines in data: {}\nNumber of real Headlines in data: {}\n'''.format(\n    counts[1],\n    counts[0]))\n\nfig = go.Figure(data=[go.Pie(labels=['Real_News','Clickbait'], values=counts, hole=.3)])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Difference: ',counts[0]-counts[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['headline'].values\nlabels = df['clickbait'].values\ntext_train, text_test, y_train, y_test = train_test_split(text, labels, test_size=0.2)\nprint(text_train.shape, text_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 5000\nmaxlen = 500\nembedding_size = 32\n\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(text)\n\nX_train = tokenizer.texts_to_sequences(text_train)\nx_test = tokenizer.texts_to_sequences(text_test)\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=maxlen))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=1e-4,)\nckpt = ModelCheckpoint(filepath='model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=512, validation_data=(x_test, y_test), epochs=1000, callbacks=[es, ckpt, rlp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nx = range(1, len(acc) + 1)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(x, acc, 'b', label='Training acc')\nplt.plot(x, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(x, loss, 'b', label='Training loss')\nplt.plot(x, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = [round(i[0]) for i in model.predict(x_test)]\ncm = confusion_matrix(y_test, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\nplt.yticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = cm.ravel()\n\nprecision = tp/(tp+fp)\nrecall = tp/(tp+fn)\n\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = ['My biggest laugh reveal ever!', 'Learning game development with Unity', 'A tour of Japan\\'s Kansai region', '12 things NOT to do in Europe']\ntoken_text = pad_sequences(tokenizer.texts_to_sequences(test), maxlen=maxlen)\npreds = [round(i[0]) for i in model.predict(token_text)]\nfor (text, pred) in zip(test, preds):\n    label = 'Clickbait' if pred == 1.0 else 'Not Clickbait'\n    print(\"{} - {}\".format(text, label))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}